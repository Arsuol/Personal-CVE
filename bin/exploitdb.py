__author__ = "Arthur Tressuol"
__email__ = "arthur.tressuol@gmail.com"
__credits__ = "Arthur Tressuol"
__date__ = "February 2022"
__revision__ = "1.0"

import time
import os
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from prettytable import PrettyTable

def exploitdb(cve_id):
    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))

    #Get search page
    url = "https://www.exploit-db.com/search?cve=" + cve_id
    driver.get(url)
    time.sleep(5)
    
    ##Print html page to parse with beautiful soup
    tmp_file = "./tmp.html"
    with open(tmp_file, 'w') as f:
        f.write(driver.page_source)
    
    driver.quit()
    
    #Parse with beautiful soup
    with open(tmp_file, 'r') as f:
        content = f.read()
    soup = BeautifulSoup(content, 'html.parser')
    
    #Get relevant information
    table = PrettyTable(['Date', 'Title', 'Type', 'url'])
    table.align = "l"
    tmp = str(soup.find("div", {"id": "exploits-table_wrapper"})).split('\n')
    tmp_str = tmp[4]
    while 1:
        index1 = tmp_str.find('href="/exploits')
        if index1 == -1:
            break
        index2 = tmp_str[index1+7:].find('"')
        exploit_link = 'https://www.exploit-db.com/' + tmp_str[index1+7:index1+7+index2]
        index2 = tmp_str.find('tabindex="0">')
        exploit_date = tmp_str[index2+13:index2+23]
        index2 = tmp_str[index1:].find('>')+1
        index3 = tmp_str[index1:].find('<')
        exploit_title = tmp_str[index1+index2:index1+index3]
        index2 = index1 + index3 + 12
        index2 += tmp_str[index2:].find('>')+1
        index3 = tmp_str[index2:].find('<')
        exploit_type = tmp_str[index2:index2+index3]
        table.add_row([exploit_date, exploit_title, exploit_type, exploit_link])
        tmp_str = tmp_str[index1+5:]
    print(table)

    #remove the temp file
    os.remove(tmp_file)
