#from bs4 import BeautifulSoup
#import requests
#import wget
#import os
import time
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from prettytable import PrettyTable

cve_id = 'CVE-2021-44228'

driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))

#Get search page
url = "https://www.exploit-db.com/search?cve=" + cve_id
driver.get(url)
time.sleep(5)

##Print html page to parse with beautiful soup
with open("./tmp.html", 'w') as f:
    f.write(driver.page_source)

driver.quit()

#Parse with beautiful soup
with open("./tmp.html", 'r') as f:
    content = f.read()
soup = BeautifulSoup(content, 'html.parser')

#Get relevant information
table = PrettyTable(['Date', 'Title', 'Type', 'url'])
table.align = "l"
tmp = str(soup.find("div", {"id": "exploits-table_wrapper"})).split('\n')
tmp_str = tmp[4]
while 1:
    index1 = tmp_str.find('href="/exploits')
    if index1 == -1:
        break
    index2 = tmp_str[index1+7:].find('"')
    exploit_link = 'https://www.exploit-db.com/' + tmp_str[index1+7:index1+7+index2]
    index2 = tmp_str.find('tabindex="0">')
    exploit_date = tmp_str[index2+13:index2+23]
    index2 = tmp_str[index1:].find('>')+1
    index3 = tmp_str[index1:].find('<')
    exploit_title = tmp_str[index1+index2:index1+index3]
    index2 = index1 + index3 + 12
    index2 += tmp_str[index2:].find('>')+1
    index3 = tmp_str[index2:].find('<')
    exploit_type = tmp_str[index2:index2+index3]
    table.add_row([exploit_date, exploit_title, exploit_type, exploit_link])
    tmp_str = tmp_str[index1+5:]
print(table)
